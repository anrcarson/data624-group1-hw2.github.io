```{r, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

loadPkg <- function(x) {
  if(!require(x, character.only = T)) install.packages(x, dependencies = T, repos = "http://cran.us.r-project.org")
  require(x, character.only = T)
}

loadPkg("mlbench")
data(Glass)
str(Glass)
```

# HW3


## 3.1
![](./week3/3.1.png)

![](./week3/3.1cont.png)

### 3.1a
![](./week3/3.1a.png)

  Answer:
  I use histograms to understand the distribution of each predictor variable.  The variables differ quite a bit.  Some are more normally distributed (e.g., Na, Al) while others do not look normal at all (e.g., Ba, Fe, K).
  
```{r}
#predictor distributions.  examples.
for(i in c(2, 6, 9)){
  hist(Glass[,i], main = names(Glass)[i], xlab = names(Glass)[i], breaks = 20)
}
```

I use a correlation plot to help me understand the correlations between predictors.  There are some strong positive relationships (i.e., Rl and Ca, Al and Ba) as well as some strong negative relationships (i.e., Rl and Si, Rl and Al, Mg and Ba).  Most relationships are not very strong.

```{r, message=FALSE}
#relationships between predictors
loadPkg("corrplot")
correlations <- cor(Glass[,1:9])
corrplot(correlations)

```


### 3.1b
![](./week3/3.1b.png)

Answer: Yes, there do appear to be outliers.  "K" has a very obvious outlier with a value of 6. "Ba" also has outliers at above 2.0,and "Fe" has an outlier above 0.5.  Skew is also present in many predictors.  While some have only minor skew (e.g., Rl, Al), others are much more pronounced and obvious (e.g., Mg, Ba, Fe)




### 3.1c
![](./week3/3.1c.png)

Answer: Yes, a log or Box Cox transformation could help remove the skew mentioned above.  Depending on what kind of classification model we are using, centering and scaling could be important for all variables.  For example, a logistic regression classification type model will be much more sensitive to variables on different scales than a decision tree.  Removing the outliers may still be required after addressing skew, so that may be needed as well to improve model performance.  Thankfully, there are no missing values in any columns, so we do not need to address those by imputation, removal, or other means.  




## 3.2
![](./week3/3.2.png)

![](./week3/3.2cont.png)

```{r}
loadPkg("mlbench")
data(Soybean)
```


### 3.2a
![](./week3/3.2a.png)

Answer: The frequency distributions for the categorical predictors are degenerate.  Most consist of two or three values, so the distributions are not normal.  This is not surprising as the values are categorical, not continuous.  Often the values in the variables are not evenly distributed by frequency, with one or more values having a much greater frequency than others (e.g., leaf.marg has many 0 and 2 but few 1).  
  
```{r, message=FALSE, warning=FALSE}
#frequency distributions.  examples
for(i in c(4,13,15)){
  plot(Soybean[,i], main = names(Soybean)[i])
}

```

There are lots of missing values in the data set as a whole, and nearly every variable has missing values.  

```{r, message=FALSE, warning=FALSE}
#summary.  show missing value counts
summary(Soybean[,2:36])
```

Furthermore, once the variables are transformed into dummy variables, there are clear cases of collinearity (e.g., roots value "2" has 0.96 correlation with fruit pods value "2", shriveling value "1" has 0.86 correlation with seed size value "1").  So the data has lots of issues from a modeling perspective.

```{r, message=FALSE, warning=FALSE}
#collinearity
loadPkg("caret")
soy_dummy_model <-dummyVars(~., data=Soybean[,2:36])
soy_dummy <-predict(soy_dummy_model, Soybean[,2:36])
soy_dummy<-data.frame(soy_dummy)

#correlation plot
corr_soy<- cor(soy_dummy, use = "pairwise.complete.obs")
corrplot(corr_soy)
  
```


### 3.2b
![](./week3/3.2b.png)

  Answer:
  Yes, there are particular predictors that are more likely to be missing.  A count of NAs below shows that we have counts of NAs in each column ranging from 0 through 121.  A distribution of the NA counts shows that it not normally distributed, and that there are gaps in the middle of the range (40 to 80).
  
```{r, message=FALSE, warning=FALSE}
loadPkg("dplyr")

#get counts of NAs
df_na <-c()
for(i in 2:36){
  name <- names(Soybean)[i]
  count <-sum(is.na(Soybean[,i]))
  row <-c(name, count)
  df_na <- rbind(df_na, row)
}
df_na <- data.frame(df_na, row.names = NULL, stringsAsFactors = FALSE)
names(df_na)<-c("Variable", "NA_Count")
df_na$NA_Count <- as.integer(df_na$NA_Count)
head(arrange(df_na, desc(NA_Count)), n=10)

#hist
hist(df_na$NA_Count, breaks = 10)

```
  
  Is the pattern of missing data related to the classes?  There are 19 classes, and there is definitely a pattern related to the classes. The classes below have repeated counts of missing values across multiple variables:
  
  * 2-4-d-injury: 16
  * cyst-nematode: 14
  * diaporthe-pod-&-stem-blight: 15
  * herbicide-injury: 8
  * phytophthora-rotL: 68 or 55
  
```{r, message=FALSE, warning=FALSE}
#get NAs by class
Soybean_NA <-is.na(Soybean)
Soybean_NA[Soybean_NA==TRUE] <-1
Soybean_NA <- data.frame(Soybean_NA)
Soybean_NA$Class <- as.character(Soybean$Class)

#group to get count of NAs per column by class value
Soybean_NA_grouped <- Soybean_NA %>% group_by(Class) %>%
  summarise_all(funs(sum(.)))
Soybean_NA_grouped

```
In fact, these 5 classes are the only classes that have missing values.
  
```{r, message=FALSE, warning=FALSE}
#calculate totals of NAs by class
totals <-rowSums(Soybean_NA_grouped[,2:36])
Soybean_NA_totals <- data.frame(cbind(Soybean_NA_grouped$Class, totals), stringsAsFactors = FALSE)
names(Soybean_NA_totals) <-c("Class","Totals")
Soybean_NA_totals$Totals <- as.integer(Soybean_NA_totals$Totals)
head(arrange(Soybean_NA_totals, desc(Totals)), n=10)

```



### 3.2c
![](./week3/3.2c.png)

Answer:  Since missing values are related to particular classes, we do not want to remove rows with missing values, as this would be to remove information that could predict the appropriate class.  Furthermore, if we did that, we might remove all or nearly all of the rows that have a particular class.  For example, there are 16 rows with a class of "2-4-d-injury".  And there are 16 rows that are missing values in the "plant stand" column that have a class of "2-4-d-injury".  So if we removed all of these, we would be removing every instance of "2-4-d-injury", which we certainly do not want to do.
 
```{r, message=FALSE, warning=FALSE}
#counts of rows by class
summary(Soybean$Class)
```

I would not want to remove predictors either, as all of the predictors have some number of missing values.  The only case in which I would be comfortable with removing predictors would be if predictors were highly correlated and I could remove one without losing information.  However, the remaining predictor would likely still have lots of missing values that would need to be addressed.

Consequently, I would do two things.  First, I would make additional predictors based on whether a value was missing or not for each original predictor.  For example, I would add a "Hail_NA" column which would have a 1 if the value was missing and a 0 if it was not for every row in the data set.  The addition of such predictors would be very useful in a decision tree model, as the presence of any missing values reduces the class options down to 5, and a split on having missing hail values but no missing precip values reduces the options to 2, and finally, a split on no missing plant stand values reduces the options to 1.

Such additional NA predictor columns may prove useful for a decision tree model, but are likely less important for a logistic regression classifier.  Thus, I would want to handle the missing values using imputation as well, as this approach will work better for logistic regression models. As mentioned in the book, a kNN model would probably do a good job of filling in the missing values.  This would find all the nearest points and impute the categorical value that is most often present in these nearest points.  See example below:
  
```{r, message=FALSE, warning=FALSE}
#https://cran.r-project.org/web/packages/VIM/VIM.pdf
loadPkg("VIM")

#show incomplete cases
Soybean[which(!complete.cases(Soybean))[0:5],]

#impute
Soybean_impute <- kNN(Soybean,  useImputedDist = FALSE, imp_var = FALSE)
Soybean_impute[which(!complete.cases(Soybean))[0:5],]

```
  Finally, it might make more sense to combine these two approaches and impute a value that indicates that the value was missing.  That is, we treat a missing value as a category of its own.  Imputing a "-1" would work fine as the predictors are all categorical, and as long as we are not using the ordinality in the factors, a "-1" value won't mess up the model.  This also avoids the need to have additional predictors and preserves the information that a missing value contains within the data set.

```{r, message=FALSE, warning=FALSE}
#impute -1
Soybean_impute_Neg1 <- Soybean
Soybean_impute_Neg1 <- data.frame(lapply(Soybean_impute_Neg1, as.character), stringsAsFactors = FALSE)
Soybean_impute_Neg1[is.na(Soybean_impute_Neg1)] <- "-1"

#compare
Soybean_impute_Neg1[which(!complete.cases(Soybean))[0:5],]

```

I would try all of these approaches and select the one that works the best for the chosen models.